{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tweepy\n",
    "pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tweepy as tw\n",
    "import string\n",
    "import nltk\n",
    "from langdetect import detect\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### DATA MINING ######\n",
    "\n",
    "#variable names you need to set to your own\n",
    "datafile = \"2020-03-31-dataset.tsv\"\n",
    "name_of_file = \"contentOfTweet_mar31\"\n",
    "\n",
    "#These are necassery to access the twitterAPI\n",
    "API_KEY = \"UgIsUxFWl6jPmGALU5oISJ4CD\"\n",
    "API_SECRET = \"tEpI8kTz3sITys0dS4VESn3VuMi6ca6D9aBjCfYJws6f6QxHa3\"\n",
    "ACCESS_TOKEN = \"1260975277343195139-kmZlvSlyoRUbkwkmy6dsX6SBteXO4Z\"\n",
    "ACCESS_TOKEN_SECRET = \"6VRmGTFNxbAqogTKXirDEi9y3KIUwiFtnhhAXK2kzAPsQ\"\n",
    "\n",
    "#Load in the specific dataset\n",
    "all_tweets = pd.read_csv(datafile, sep='\\t')\n",
    "\n",
    "#Upper bound on the amount of tweets\n",
    "max_tweets = all_tweets.shape[0]\n",
    "\n",
    "#auth = tw.AppAuthHandler(API_KEY, API_SECRET)\n",
    "auth = tw.OAuthHandler(API_KEY, API_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "api = tw.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "#check if there are more than 50000 tweets in the dataset\n",
    "#if there is randomize the order of the tweets\n",
    "if all_tweets.shape[0] > 30000:\n",
    "    all_tweets = all_tweets.sample(frac=1).reset_index(drop=True)\n",
    "    max_tweets = 30000\n",
    "    \n",
    "#get the text of the tweet per id  \n",
    "for index in range(max_tweets):\n",
    "    tid = all_tweets.iloc[index]['tweet_id']\n",
    "    try:\n",
    "        tweet    = api.get_status(tid) \n",
    "        tweetxt  = tweet.text\n",
    "        language = detect(tweetxt)\n",
    "        all_tweets.loc[all_tweets.index[index], 'content']  = tweetxt\n",
    "        all_tweets.loc[all_tweets.index[index], 'language'] = language\n",
    "    \n",
    "    #For if the tweet has been deleted\n",
    "    except: \n",
    "        all_tweets.loc[all_tweets.index[index], 'content'] = 'deleted'\n",
    "      \n",
    "    if (index % 100) == 0:\n",
    "        print(\"{} {}\".format(index, \"tweets done\"))\n",
    "        \n",
    "#save to a csv file        \n",
    "all_tweets.to_csv(name_of_file)\n",
    "\n",
    "\n",
    "###### SENTIMENT ANALYSIS ######\n",
    "\n",
    "#name of dedicated files\n",
    "file = \"contentOfTweet_apr21\"\n",
    "sentimentfile = 'apr21_with_sentiment'\n",
    "\n",
    "#open file with tweets\n",
    "all_languages = pd.read_csv(file, sep=',')\n",
    "\n",
    "#Open the sentiment lexicon as a dataframe\n",
    "lexicon = pd.read_csv ('sentiment_lexicon.csv', sep=';')\n",
    "\n",
    "#Make a set of words from the lexicon\n",
    "word_list = set(lexicon['word'])\n",
    "\n",
    "#filter the tweets on english only\n",
    "only_english = all_languages.loc[all_languages['language'] == 'en']\n",
    "only_english = only_english.reset_index(drop=True)\n",
    "\n",
    "print('Done with filtering on english')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "for indx, row in only_english.iterrows():\n",
    "    twt = only_english.iloc[indx]['content']\n",
    "    twt = ' '.join(word for word in twt.split(' ') if not word.startswith('http')) #Remove URL's\n",
    "    twt = ' '.join(word for word in twt.split(' ') if not word.startswith('@'))    #Remove @ with the user ID\n",
    "    twt = twt.replace('RT', '')                                                    #Remove the RT statement\n",
    "    \n",
    "    #POS tag the sentence\n",
    "    tex  = word_tokenize(twt)\n",
    "    tags = nltk.pos_tag(tex)\n",
    "    newlist = []\n",
    "    \n",
    "    for word, tag in tags:\n",
    "        \n",
    "        #If the word is a stop word do nothing\n",
    "        if not word in cachedStopWords:\n",
    "            \n",
    "            #If the word is not in the lexicon we check if the word's stem IS in the lexicon, if so we replace the word with it's stem\n",
    "            if not word in word_list:\n",
    "\n",
    "                #Take the stem if the word is verb\n",
    "                if tag in ('VBD','VBG','VBN','VBP','VBZ'):\n",
    "                    vstem = lemmatizer.lemmatize(word,'v')\n",
    "                    newlist.append(vstem)\n",
    "                \n",
    "                #Take the stem if the word is a nn\n",
    "                elif tag in ('NNS', 'NNPS'):\n",
    "                    nstem = lemmatizer.lemmatize(word, 'n')\n",
    "                    newlist.append(nstem)\n",
    "                    \n",
    "            #if the word is in the lexicon then we don't need to alter the word form\n",
    "            else:\n",
    "                    newlist.append(word)\n",
    "    \n",
    "    #Make a string from the list of words\n",
    "    twt = \" \".join(newlist)\n",
    "    \n",
    "    twt = twt.translate(str.maketrans('', '', string.punctuation))        #Remove puntuations\n",
    "    only_english.loc[only_english.index[indx], 'content']  = twt.lower()  #lower case everything\n",
    "\n",
    "print('Done with NLP')\n",
    "    \n",
    "#Add new columns for every emotions to be detected\n",
    "number_of_rowsZero = [0]*only_english.shape[0]\n",
    "only_english['anger']        = number_of_rowsZero\n",
    "only_english['anticipation'] = number_of_rowsZero\n",
    "only_english['disgust']      = number_of_rowsZero\n",
    "only_english['fear']         = number_of_rowsZero\n",
    "only_english['joy']          = number_of_rowsZero\n",
    "only_english['sadness']      = number_of_rowsZero\n",
    "only_english['surprise']     = number_of_rowsZero\n",
    "only_english['trust']        = number_of_rowsZero\n",
    "\n",
    "#Loop over the tweet dataframe\n",
    "#Get the content of the tweet and put it into a list of words\n",
    "for index, row in only_english.iterrows():\n",
    "    twt = only_english.iloc[index]['content']\n",
    "    twt = twt.split()\n",
    "    \n",
    "    #Check if the word of the tweet can be found somewhere in the sentiment lexicon\n",
    "    for word in twt:\n",
    "        #When the word is not in the list it does not have any sentiment and thus nothing to do with it\n",
    "        if word in word_list:\n",
    "            word_index = lexicon[lexicon['word']==word].index.values.astype(int)[0]\n",
    "            emotions   = lexicon.iloc[word_index]['emotion']\n",
    "            score      = lexicon.iloc[word_index]['emotion-intensity-score']\n",
    "\n",
    "            #Check to which emotion the word corresponds and add up the score of that word to the already \n",
    "            #excisting score of the tweet\n",
    "            if emotions   == 'anger':\n",
    "                score_so_far = only_english.iloc[index]['anger']\n",
    "                only_english.loc[only_english.index[index], 'anger']  = score_so_far + score\n",
    "            elif emotions == 'anticipation':\n",
    "                score_so_far = only_english.iloc[index]['anticipation']\n",
    "                only_english.loc[only_english.index[index], 'anticipation']  = score_so_far + score\n",
    "            elif emotions == 'disgust':\n",
    "                score_so_far = only_english.iloc[index]['disgust']\n",
    "                only_english.loc[only_english.index[index], 'disgust']  = score_so_far + score\n",
    "            elif emotions == 'fear':\n",
    "                score_so_far = only_english.iloc[index]['fear']\n",
    "                only_english.loc[only_english.index[index], 'fear']  = score_so_far + score\n",
    "            elif emotions == 'joy':\n",
    "                score_so_far = only_english.iloc[index]['joy']\n",
    "                only_english.loc[only_english.index[index], 'joy']  = score_so_far + score\n",
    "            elif emotions == 'sadness':\n",
    "                score_so_far = only_english.iloc[index]['sadness']\n",
    "                only_english.loc[only_english.index[index], 'sadness']  = score_so_far + score\n",
    "            elif emotions == 'surprise':\n",
    "                score_so_far = only_english.iloc[index]['surprise']\n",
    "                only_english.loc[only_english.index[index], 'surprise']  = score_so_far + score\n",
    "            else:\n",
    "                score_so_far = only_english.iloc[index]['trust']\n",
    "                only_english.loc[only_english.index[index], 'trust']  = score_so_far + score\n",
    "\n",
    "print('Done with emotion detection')\n",
    "                \n",
    "#Save the dataframe with sentiment score in a new file\n",
    "only_english.to_csv(sentimentfile)\n",
    "\n",
    "###### DATA PREPROCESSING FOR VISUALIZATION ######\n",
    "\n",
    "sentiment_aprl = pd.read_csv('Sentiment_FebMarchAprilMay', sep=',')\n",
    "\n",
    "date_and_sentiment = sentiment_aprl[['date','anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust']]\n",
    "\n",
    "av_per_day = date_and_sentiment.groupby(['date'], as_index=False).mean()\n",
    "\n",
    "\n",
    "for index, row in av_per_day.iterrows():\n",
    "    datestring = row['date']\n",
    "    month      = datestring[5]\n",
    "    day        = int(datestring[-2:])\n",
    "    \n",
    "    if day < 0:\n",
    "        day = day *-1\n",
    "        \n",
    "    av_per_day.loc[index, 'day'] = day\n",
    "    av_per_day.loc[index, 'month']  = month\n",
    "av_per_day.sort_values(by=['month', 'day'], inplace=True)  \n",
    "av_per_day = av_per_day.reset_index(drop=True)\n",
    "print(av_per_day)\n",
    "\n",
    "###### VISUALIZATION ######\n",
    "\n",
    "av_per_day.plot.line(x= 'date', y=['anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust'])\n",
    "plt.title('Mean sentiments per day')\n",
    "plt.ylabel('Sentiment score')\n",
    "plt.xlabel('Date')\n",
    "plt.xticks(rotation= 65)\n",
    "plt.legend(loc=(1.05, 0.4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
